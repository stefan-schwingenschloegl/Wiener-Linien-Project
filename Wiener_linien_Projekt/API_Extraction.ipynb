{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Extraction <a class=\"tocSkip\">\n",
    "Author: Stefan Roland Schwingenschlögl <br>\n",
    "email: stefan.roland.schwingenschloegl@gmail.com <br>\n",
    "github: github.com/stefan-schwingenschloegl <br>\n",
    "___\n",
    "*Projekt File No: 4 <br>*\n",
    "    \n",
    "This file is about requesting the live data from the API and extracting the relevant response information in json format. This relevant data is then transferred to the database immediately after the query and stored there. The reason for this is that the data should be transferred to a persistent memory as quickly as possible. If the data were first stored in RAM as a pandas dataframe, the data would be lost in the event of a technical failure (e.g. power failure). On the database server, a power failure during the extraction phase would have no effect on the already loaded data. An alternative would be to save each individual response as a csv. However, all flat files would then have to be read in individually via a loop and then concatinated. In this case, access via SQL is definitely more convenient.  <br>\n",
    "    \n",
    "The collected data, which is extracted from the response in json format, is as follows:\n",
    "* `timestamp`: timestamp of the Wiener Linien server of the query.\n",
    "* `stop_name`: Human readable name of the stop\n",
    "* `line_name`: Human readable name of the line\n",
    "* `towards`: Human readable name of the final stop\n",
    "* `directionID`: Unique Identifier of the direction of the line\n",
    "* `type`: Type of vehicle\n",
    "* `timePlanned`: timestamp at which the next vehicle should arrive according to the timetable\n",
    "* timeReal': live projection of the timestamp at which the next vehicle should actually arrive\n",
    "* `DIVA`: Unique Identifier of a station area with multiple stops\n",
    "* `rbl`: Old name for `stopID`. Unique Identifier of a stop  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set General Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the path to the Wiener Linien API documentation\n",
    "wl_docu_url = 'http://www.wienerlinien.at/ogd_realtime/doku/'\n",
    "\n",
    "#set the URL to the Wiener Linien API real time monitor\n",
    "wl_monitor_url = 'http://www.wienerlinien.at/ogd_realtime/monitor?'\n",
    "\n",
    "#list of all stations which will be observed\n",
    "str_stationen = ['Kardinal-Nagl-Platz', \n",
    "                 'Margaretenplatz, Schönbrunner Straße', \n",
    "                 'Schönbrunn U', \n",
    "                 'Laxenburger Straße / Gudrunstraße']\n",
    "\n",
    "# set string to data folders\n",
    "input_folder = \"./input_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_timestamps(start_time, end_time):\n",
    "    \"\"\"    \n",
    "    Method to check if the window of the timestamps are valid.\n",
    "    restrictions:\n",
    "    - if start_time is in the past -> changed to current timestamp\n",
    "    - end_time must not be in the past\n",
    "    - start_time must not be later than end_time\n",
    "    \n",
    "    input args: start_time, end_time as string format\n",
    "    output args: start_time, end_time as datetime format\n",
    "    \"\"\"\n",
    "    \n",
    "    #check if start_time is valid | if not set it to current time\n",
    "    try:     # evaluate if input string is in the right format\n",
    "        start_time = pd.to_datetime(start_time).tz_localize('Europe/Vienna') \n",
    "        if start_time < get_current_timestamp(): #check if start_time is in past\n",
    "            raise\n",
    "        else:\n",
    "            print(f'Start time valid.')\n",
    "        \n",
    "    except: # if input string is in wrong format or in the past replace with current timestamp\n",
    "            print(f\"Your chosen timestamp({start_time.replace(tzinfo=None)}) is not valid.\\n\"\n",
    "                  f\"Maybe the string is not correct or the time is in the past\\n\"\n",
    "                  f\"Since it is the start time it got replaced with the current timestamp({get_current_timestamp().replace(tzinfo=None)}).\\n\")\n",
    "            start_time=get_current_timestamp()\n",
    "            \n",
    "    # check if input string for end_time is in the right format| if not raise error       \n",
    "    try:\n",
    "        end_time = pd.to_datetime(end_time).tz_localize('Europe/Vienna')\n",
    "        \n",
    "    except:\n",
    "        raise Exception(f\"Endtime({end_time}) is not Valid. Please enter valid time.\")\n",
    "        \n",
    "    #check if end_time is later than start_time | if not raise error\n",
    "    if start_time > end_time:\n",
    "        raise Exception(f'Starttime({start_time.replace(tzinfo=None)}) is later than Endtime({end_time.replace(tzinfo=None)})\\n'\n",
    "                        f'Please check order or enter valid time.')\n",
    "    else:\n",
    "        print(f'End time valid.')\n",
    "        return start_time, end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate current timestamp\n",
    "def get_current_timestamp(granularity = \"min\"):\n",
    "    \"\"\"    \n",
    "    Get timestamp from system:\n",
    "    - if granularity == 'sec' -> timestamp in hour:min:seconds\n",
    "    - else -> timestamp in hour:min\n",
    "    \n",
    "    input args: granularity as string format (default 'min')\n",
    "    output args: timestamp in datetime format\n",
    "    \"\"\"\n",
    "    if granularity == \"sec\":\n",
    "        return pd.to_datetime(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")).tz_localize('Europe/Vienna')\n",
    "    else:\n",
    "        return pd.to_datetime(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")).tz_localize('Europe/Vienna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get DIVA Numbers of all stations\n",
    "___\n",
    "Since the user usually does not know the DIVA number, but the names of the various stops, the DIVA numbers must first be searched for. This is done by using SQL to query all entries from the previously filled `haltepunkte` table in the database. These DIVA numbers are then stored in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set properties for database\n",
    "db_name = 'WienerLinienDB'\n",
    "server = 'DESKTOP-JV1HTQR\\SQLEXPRESS'\n",
    "db_connection = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish DB Connection\n",
    "def db_connect(server, db_name):\n",
    "    \"\"\"\n",
    "    Establish database connection\n",
    "    \n",
    "    input args: server, db_name as string format\n",
    "    return args: database connection object\n",
    "    \"\"\"\n",
    "    conn = pyodbc.connect(\"driver={SQL Server};server=\"+server+\"; database=\"+db_name+\"; trusted_connection=true\")\n",
    "    db_connection = True\n",
    "   # print(f\"\\nConnection with {server} sucessfull!\\n\"\n",
    "    #      f\"Current Database: {db_name}\\n\"\n",
    "    #      f\"DB Connection Status: {db_connection}\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close DB Connection\n",
    "def close_connection(server, conn):\n",
    "    conn.close()\n",
    "    db_connection = False\n",
    "   # print(f\"\\nDB-Connection with Server {server} closed.\\n\"\n",
    "   #       f\"DB Connections Status: {db_connection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DIVA(str_stationen):\n",
    "    conn = db_connect(server, db_name)\n",
    "    \n",
    "    with conn:\n",
    "        df = pd.read_sql(f\"select * from haltepunkte where StopText in \" + str(tuple(str_stationen)), conn)\n",
    "    \n",
    "    close_connection(server, conn)\n",
    "    \n",
    "    return df.loc[df['StopText'].isin(str_stationen),'DIVA'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIVA_list = get_DIVA(str_stationen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Request String for API Call\n",
    "___\n",
    "After the DIVA numbers are known, the next step is to automatically generate the query string from all elements of the DIVA_list for the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the request for the API call from the generated dataframe\n",
    "def build_request(DIVA_list):\n",
    "    request = wl_monitor_url\n",
    "    \n",
    "    for DIVA in DIVA_list:\n",
    "        request =  request + 'diva='+ DIVA.astype('int').astype('str') +'&'\n",
    "    \n",
    "    return request[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.wienerlinien.at/ogd_realtime/monitor?diva=60200829&diva=60201211&diva=60200136&diva=60200653'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the request for all stored stations in DIVA list\n",
    "request = build_request(DIVA_list)\n",
    "request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Live Data\n",
    "___\n",
    "This is where the magic happens. The process of an automated request via the API is as follows:\n",
    "1. The User passes the request string and the desired start and end time to the 'scheduling' function. If the start time is still in the future, a 'countdown' is started on a seconds basis. As soon as the start time has been reached, the API calls are made at intervals of 1 minute. The reason for this interval is that the server should not be overloaded. \n",
    "2. The response of this API call is stored in json format in a variable.\n",
    "3. The desired information is extracted from this variable and stored in a pandas dataframe. \n",
    "4. The values from this pandas dataframe are inserted into the database using SQL.\n",
    "5. If the whole process is successful, the timestamp is stored in a log_dictionary, and the timestamps of unsuccessful queries are stored under a different key in the log_dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na values in columns with datetime with the highest possible value \n",
    "# fill na values in delay with -100000\n",
    "def fill_na(df):\n",
    "    for col in df.columns:\n",
    "        if 'time' in col:\n",
    "            df[col] = df[col].fillna(\"9999-12-31 23:59:59+01:00\")\n",
    "        elif col == 'delay':\n",
    "            df[col] = df[col].fillna(-100000)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information from Wiener Linien API \n",
    "def api_call(request):\n",
    "    \n",
    "    #get json data from Wiener Linien API call\n",
    "    api_json = requests.get(request).json()\n",
    "    \n",
    "    #pick relevant info from received json and convert it to dataframe\n",
    "    return translate_json_to_df(api_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_json_to_df(entry):\n",
    "    \"\"\"\n",
    "    Method to extract relevant data from API response and write into pandas dataframe\n",
    "    \n",
    "    input_args: entry (json format)\n",
    "    returns: df (pandas dataframe)\n",
    "    \"\"\"\n",
    "    data = entry['data']['monitors']\n",
    "    \n",
    "    # dictionary with all relevant categories as keys\n",
    "    station = dict({'timestamp': [],\n",
    "                    'stop_name': [],\n",
    "                    'line_name': [],\n",
    "                    'lineID' : [],\n",
    "                    'towards': [],\n",
    "                    'richtungsID': [],\n",
    "                    'type': [],\n",
    "                    'timePlanned': [],\n",
    "                    'timeReal': [],\n",
    "                    'DIVA': [],\n",
    "                    'rbl': []\n",
    "                    })\n",
    "    \n",
    "    for line in range(0, len(data)):\n",
    "        \n",
    "        # pick the relevant information and write it to the dictionary\n",
    "        for line_plat in range(0, len(data[0]['lines'])):\n",
    "            station['timestamp'].append(pd.to_datetime(entry['message']['serverTime']))\n",
    "            station['stop_name'].append(data[line]['locationStop']['properties']['title'])\n",
    "            station['DIVA'].append(data[line]['locationStop']['properties']['name'])\n",
    "            station['rbl'].append(data[line]['locationStop']['properties']['attributes']['rbl'])\n",
    "            station['line_name'].append(data[line]['lines'][line_plat]['name'])\n",
    "            station['type'].append(data[line]['lines'][line_plat]['type'])\n",
    "            station['towards'].append(data[line]['lines'][line_plat]['towards'])\n",
    "            station['richtungsID'].append(data[line]['lines'][line_plat]['richtungsId'])\n",
    "            station['lineID'].append(data[line]['lines'][line_plat]['lineId'])\n",
    "\n",
    "            if (data[line]['lines'][line_plat]['realtimeSupported']):\n",
    "                try:\n",
    "                    station['timePlanned'].append(pd.to_datetime(data[line]['lines'][line_plat]['departures']['departure'][0]['departureTime']['timePlanned']))\n",
    "                    station['timeReal'].append(pd.to_datetime(data[line]['lines'][line_plat]['departures']['departure'][0]['departureTime']['timeReal']))\n",
    "                except:\n",
    "                    station['timePlanned'].append(np.nan)\n",
    "                    station['timeReal'].append(np.nan)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # convert ditionary to dataframe\n",
    "    station_df = pd.DataFrame(station)\n",
    "    \n",
    "    # calculate the delay in seconds\n",
    "    station_df['delay'] = (station_df['timeReal'] - station_df['timePlanned']).dt.total_seconds()\n",
    "    \n",
    "    station_df = fill_na(station_df)\n",
    "    \n",
    "    return station_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_stage(df, server, db_name):\n",
    "    \"\"\"\n",
    "    Method to insert relevant data from API call into DB\n",
    "    \n",
    "    input_args: df (dataframe format), server (string format), db_name (string format)\n",
    "    returns: - \n",
    "    \"\"\"\n",
    "    \n",
    "    conn = db_connect(server=server, db_name=db_name)\n",
    "    sql_command = '''\n",
    "                INSERT INTO [dbo].[stage_delay]\n",
    "                           ([timestamp]\n",
    "                           ,[stop_name]\n",
    "                           ,[line_name]\n",
    "                           ,[lineID]\n",
    "                           ,[towards]\n",
    "                           ,[richtungsID]\n",
    "                           ,[type]\n",
    "                           ,[timePlanned]\n",
    "                           ,[timeReal]\n",
    "                           ,[DIVA]\n",
    "                           ,[rbl]\n",
    "                           ,[delay])\n",
    "                     VALUES\n",
    "                           (?,\n",
    "                            ?,\n",
    "                            ?,\n",
    "                            ?,\n",
    "                            ?,\n",
    "                            ?,\n",
    "                            ?,\n",
    "                            ?,\n",
    "                            ?,\n",
    "                            ?,\n",
    "                            ?,\n",
    "                            ?)\n",
    "                '''\n",
    "    with conn:\n",
    "        crs=conn.cursor()\n",
    "        #try:\n",
    "        for index, row in df.iterrows():\n",
    "            crs.execute(sql_command, row['timestamp'],\n",
    "                                     row['stop_name'],\n",
    "                                     row['line_name'],\n",
    "                                     row['lineID'],\n",
    "                                     row['towards'],\n",
    "                                     row['richtungsID'],\n",
    "                                     row['type'],\n",
    "                                     row['timePlanned'],\n",
    "                                     row['timeReal'],\n",
    "                                     row['DIVA'],\n",
    "                                     row['rbl'],\n",
    "                                     row['delay']\n",
    "                                     )\n",
    "        #print(f'{get_current_timestamp()}: sucessfully filled!')\n",
    "        #except:\n",
    "            #print('Not Sucessfull!')\n",
    "    close_connection(server = server, conn=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(start_time, end_time, request):\n",
    "    \"\"\"\n",
    "    Scheduling workflow to automate API calls.\n",
    "    while start_time < current_time: (Countdown to start)\n",
    "        sleep for one second and then check again\n",
    "    \n",
    "    while current_time <= end_time: (API Calls)\n",
    "        if second == 0:\n",
    "            call API with request and write it into DB\n",
    "        else:\n",
    "            sleep 1 second and check again\n",
    "    \n",
    "    input_args: start_time, end_time, request as string format\n",
    "    returns: dictionary with timestamps of sucessfull and not sucessfull loads \n",
    "    \"\"\"\n",
    "    data_df=pd.DataFrame()\n",
    "    time_start = get_current_timestamp(\"min\")\n",
    "    log_dict = {'successfull': [],\n",
    "                'failed': []}\n",
    "    success_counter = 0\n",
    "    failed_counter = 0\n",
    "    \n",
    "    print(f'######## Start Countdown Begin ########\\n')\n",
    "    print(f'Started Job: {time_start.replace(tzinfo=None)}\\n'\n",
    "          f'Start with API Calls: {start_time.replace(tzinfo=None)}\\n')\n",
    "    \n",
    "    # check if start_time is in the future | if so 1 second delay; if not go on with data collection\n",
    "    while start_time > get_current_timestamp():\n",
    "        time_now = get_current_timestamp(\"sec\")\n",
    "        print('Start Countdown: '+ str(start_time - time_now), end= '\\r')\n",
    "        time.sleep(1)\n",
    "    print(f'######## Start Countdown End ########\\n\\n'\n",
    "          f'######## Data Collection Start ########\\n')\n",
    "    \n",
    "    time_now = get_current_timestamp()\n",
    "    \n",
    "    # make first API Call and write into DB\n",
    "    try:\n",
    "        write_data_to_stage(api_call(request), server, db_name)\n",
    "        log_dict['successfull'].append(time_now)\n",
    "    except:\n",
    "        print(f'{get_current_timestamp()}: Error occured!')\n",
    "        log_dict['failed'].append(time_now)\n",
    "        \n",
    "    print(f'Start with API Calls: {get_current_timestamp().replace(tzinfo=None)}\\n'\n",
    "          f'End Time: {end_time.replace(tzinfo=None)}\\n')\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # check if current time is the endtime | if so end Data collection ; if not get entries from API and wait 1 Minute between calls  \n",
    "    while get_current_timestamp() <= end_time:\n",
    "        time_now = get_current_timestamp(\"sec\")\n",
    "        \n",
    "        if time_now.second == 0: # If new minute: call API and write to DB\n",
    "            try:\n",
    "                write_data_to_stage(api_call(request), server, db_name)\n",
    "                log_dict['successfull'].append(time_now)\n",
    "                time.sleep(1)\n",
    "\n",
    "            except:\n",
    "                log_dict['failed'].append(time_now)\n",
    "                time.sleep(1)\n",
    "                \n",
    "        else:\n",
    "            print('Sucessfull Calls: ' + str(len(log_dict['successfull'])) +' | Failed Calls: '+ str(len(log_dict['failed']))+' | Countdown to next API Call: ' + str(60 - time_now.second)+ '[s]', end='\\r')\n",
    "            time.sleep(1)\n",
    "        \n",
    "        if (time_now == end_time):\n",
    "            break\n",
    "    print('Sucessfull Calls: ' + str(len(log_dict['successfull'])) +' | Failed Calls: '+ str(len(log_dict['failed']))+' | Countdown to next API Call:', end='\\r')\n",
    "    print(f'\\n######## Data Collection End ########\\n')\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time valid.\n",
      "End time valid.\n",
      "######## Start Countdown Begin ########\n",
      "\n",
      "Started Job: 2021-01-02 16:32:00\n",
      "Start with API Calls: 2021-01-02 16:33:00\n",
      "\n",
      "######## Start Countdown End ########\n",
      "\n",
      "######## Data Collection Start ########\n",
      "\n",
      "Start with API Calls: 2021-01-02 16:33:00\n",
      "End Time: 2021-01-02 16:33:00\n",
      "\n",
      "Sucessfull Calls: 1 | Failed Calls: 0 | Countdown to next API Call: 1[s]]\n",
      "######## Data Collection End ########\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time, end_time = validate_timestamps(start_time='2021-01-02 16:33', end_time='2021-01-02 16:33')\n",
    "log = get_data(start_time, end_time, request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion & Next Steps\n",
    "___\n",
    "With this file, data was retrieved from the API of Wiener Linien on 17.12.2020 in the period from 8:30 to 11:00 and successfully written into the database. A total of 149 retrievals were started, all of which were successful. 2384 lines were inserted. This data is alos stored as `data_17_12.csv` in the `realtime_data` folder.\n",
    "\n",
    "In the next file, the entries are analyzed. This will be in the notebook `data_exploration.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
